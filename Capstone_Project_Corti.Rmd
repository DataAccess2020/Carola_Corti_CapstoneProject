---
title: "Smart cities in India: sketching public opinion trends on smart urban planning policies' narratives"
subtitle: "Capstone Project for Data Access and Regulation - Data Analytics for Politics Society and Complex Organizations, University of Milan" 
author: "Carola Corti - 961514"
date: "February 2021"
output: pdf_document
---

# Abstract

This project aims at collecting information about the discourse above smart cities. I will focus on India's specific case, inspecting whether the debate related to smart cities' policies' implementation in such country develops positively or negatively, what topics are the mostly discussed and which are the most problematic issues related to it.

The research will be composed of two main parts. In the first, I will provide sentiment and topic analyses processing a corpus of articles previews about India's smart cities development. In the second part, by interacting with the Twitter and Google Maps APIs, I will download via geographical selection the latest 2000 tweets written in India, containing the words "smart cities" and "Covid-19" .

The analyses will be run, on the one hand, in order to identify the main debated topics within the smart cities broader narratives; on the other hand, to capture the overall sentiment among the texts to draw a better picture of how smart cities are perceived in its stakeholders' opinion and what can be addressed as a criticisms or advantage in the implementation of such urban planning decisions in the India's context, taking into account both the governmental and its citizens point of view, especially in light of the Covid-19 health emergency and the known and highly debated overcrowding, poverty and diseased conditions that are common to many Indian metropolitan areas.

# Introduction

### Smart cities: definition and criticism

According to the definition provided by the European Commission, "smart cities" are places where traditional networks and services are made more efficient with the use of digital and telecommunication technologies for the benefit of its inhabitants and business. Smart metropolitan areas would do so by going beyond the use of information and communication technologies (ICT) in order to provide better resources while using less emissions. The implementation of policies aimed at providing efficient urban transport networks, upgraded water supplies, waste disposal facilities, electricity and heating technologies to let all goods reach each building and each societal stratus in the urban area, are at the heart of smart cities' goals. In other words, the underlining aim of adopting such policies would be ought to implement interactive and responsive city administration strategies, guaranteeing safer public spaces and taking into consideration the needs of the whole population.

Rob Kitchin, one of the leading scholars interested in the debate about smart cities, professor and researcher of human geography at the National Institute of Regional and Spatial Analysis at Maynooth University of Ireland, states that definition of Smart City has to take into account two important aspects. On the one side, he defines as "smart" a urban context in which *everyware* technologies (Greenfield, 2006) are massively implemented. By using this term, he refers to the massively used ubiquitous computing practices, defined by human-machine interactions extended on several levels, capable of connecting physical and digital space through the continuous collection and management of the data citizens generate interacting with the surrounding material space. On the other hand, he sheds light on the importance of understanding the relationship that occurs between citizens and the technologies they interact with. The implementation of Information Communication Technologies does not in fact constitute, in itself, a necessary and sufficient condition for a city to be defined as really intelligent; it rather is the way in which datafied interactions are used and perceived by who are supposed to be its end-users, to define whether a city is "smart" or not.

While it is easy, in fact, to build city planning projects enthusiastically focusing on the implementation of highly advanced technologies in many fields, it is less immediate to inspect the urban fabrics taking into account both the physical urban environment ( i.e. its elements, materials, form, scales, density and networks), and the psychological, socio-cultural, ecological, managerial and economic structures of metropolitan areas. The smart cities approaches have always tended, in fact, to focus on technological solutions to urban problems from the perspective of states and companies rather than critically taking into account people and communities. It is taking into account these considerations, that the debate around Smart City initiatives is more and more heated in developing countries where the ideal prerequisites to advance such policies are more likely to lack.

### Smart cities in India

This research focuses on the most prominent example In the Global South: India. This country is in fact, on the one side, a key actor in implementing national-level smart city programs, while, on the other, it constantly faces the challenges caused by an increasingly growing digital divide among its population. According to a recent World Bank report, India is the country with the largest number of non-connected citizens: data show that with a total of 1.25 billion inhabitants, more than 2billion are "offline" citizens.

However, it is precisely in India that incredibly avant-garde realities can be observed from a technological point of view (just think of the Bengaluru district, considered the Indian Silicon Valley, bulwark of technological and innovative New India). The clearest example in the urban planning context is provided by the Smart Cities Mission (SCM), a project that was launched in 2015 as a urban renewal and retrofitting program by Narendra Modi, Prime Minister of the Government of India, with the aim to develop smart cities across the country, making them citizen friendly and sustainable. The plan initially included 100 cities, planning projects to be realized between 2019 and 2023. In spite of the innovation wave the Mission wanted to follow, although extremely advanced in the technological perspective, little consideration was given to its implementation's sociological consequences.

The model of citizenship that is usually offered in smart cities contexts expects, in fact, on the one hand citizens to become "*netizens*", and, on the other, to re-configure citizenship models by instrumentalizing technology and data, reinforcing, as an unintended consequence, the patterns of exclusion for marginalized groups within the urban society, even though citizen consultation and participation are often described as the heart of smart city proposals. It is indeed not immediate for all citizens to participate actively in smart cities projects, and, whether some can, other are excluded, leading to the exacerbation of existing urban historical, material and social inequalities (Katherine S.Willis, 2019).

For instance, the city of Chennai, Located on the Coromandel Coast of the Bay of Bengal, provides clear example of the possible implications such urban planning policies have generated for marginalized communities. In Chennai, the area-based development proposals were mainly focused around the assurance of electrical and water supply, the recycling of waste water, waste management, rain water harvesting and management at households and community levels and Wi-Fi connectivity in order to integrate public transports' utilities. In spite of what could appear at first glance, as Katherine S. Willis points out in her 2019 research on India's smart cities projects, the proposed solutions, although trying to lead to concrete changes in the life-quality within the city area, failed to recognize or give value to the existing urban informality, being rather driven and rationalized by technological deterministic market plans. Urban Informality appeared to be therefore treated as a set of conditions and resources equivalent to "leaks" in a system that needed to be optimized and rationalized. As a consequence, many plans outlined in 2016 for the Smart City Chennai have failed to be realized, being even criticized through demonstrations of public protests in which banners containing writings such as "*We want bread and butter, not Smart City*" were exposed.

### Covid-19 and urban settlements in India

With an estimated 90 percent of all reported COVID-19 cases, urban areas have became the epicenter of the pandemic. The virus have, in fact, highlighted the critical role local governments play as front-line responders within the health crisis context. In this perspective, in this last year, tackling inequalities and development deficits has became of primary importance when it comes to designing policies briefs aimed at preventing the virus spread. Coronavirus has been widening existing social, spacial and economic inequalities in cities both in the Global North and in the Global South. It has in fact became clear that vulnerability to COVID- 19 depends on several conditions: where in a city a person lives and works, gender, age, income level, type of home, access to public services - such as health facilities- transportation and clean water.

Data collected by the United Nations and reported in the UN 2020 Policy Brief "*Covid-19 in an Urban World*" , suggest that the poorer neighborhoods and those with the largest households size were more likely to experience a high number of cases per capita. India, with its 35.2% of the country's whole population living in slums and informal settlements, constitutes a paradigmatic example, since many homes in such areas lack access to water and sanitation facilities, making, for example, safe and regular handwashing extremely difficult. It is in this context that the smart policies applied to urban centers are and were expected to be mainly aimed at stemming the contagion, putting in practice the technological advantage that characterize their strength.

# Research question

As for the sake of my ultimate research goal to sketch a sort of public opinion analysis regarding this debate, I will focus on topic and sentiment analyses of the corpora I will collect from the voices into play within the discourse related to smart cities in India. I will take into account two main voices: the corporate stakeholder's one, represented by the Indian newspaper **Financial Express** articles, and those coming from a more democratic medium, **Twitter**, even if aware it still not provides a fully reliable representation of the slice of non-digitalized population public opinion. I will try to provide an answer to the following questions:

-   How is sentiment (i.e. positive/negative etc.) mapped across the two main voices?

-   What are the main discussed topics between the two sides of the debate? Is there any visible difference?

-   Did the COVID-19 spread cause any change for what concerns the develop of trust/distrust in Smart Cities policies and the way Governments have faced the crisis?

# Analyzing corporate stakeholders' voice: *The Financial Express*

## Collecting articles previews from the Financial Express web page

*The Financial Express* is an Indian English -language business newspaper owned by The Indian Express Group, which has been published since 1961. The newspaper specializes in Indian and international business and finance but its articles address different actuality topics as well. An entire session is dedicated to smart cities. I chose to analyze these short texts since this newspaper can provide great evidence of the discourse around smart cities in the perspective of its political and corporate stakeholders, as the name of the newspaper suggests.

The texts were imported in R following a series of web scraping operations using the R packages *Rvest*, *Httr* and *Stringr,* in order to download textual information to build a corpus by using the *Quanteda* package and, eventually, analyze it performing text analytics. The aim, here, is to download the provided texts from pages 1 to 5 from the Financial Express website's [smart cities-dedicated session](https://www.financialexpress.com/about/smart-cities/). The following steps will report the sequence of code I ran in R to scrape the contents.

### Step 1: */robots.txt* file inspection

To check whether the website's rules permitted to perform web scraping on that page, i checked the robots.txt file. As a result, the following lines of code are displayed:

![](images/robots%20txt%20financial%20express%20.png){width="289"}

The website shows it instructions to web robots by using the Robots Exclusion Protocol. The first line tells us this section applies to all robots; the other tell no robot can crawl the listed paths. I am therefore allowed to scrape the pages I need to collect my corpus since it doesn't belong to this list.

### Step 2: Importing the data in R Studio via web scraping with *Rvest*

After having inspected the */robots.txt* file, I've selected the parts of the web page I needed to import in R using Selector Gadget to define the CSS tags corresponding to the elements. I then built a *for loop* in order to extract the main texts in each of the articles shown in the session, downloading it from the first to the last page of this session (pages 1 to 5).

```{r message=FALSE, warning=FALSE}

# Loading the required packages 

library(stringr)
library(httr)
library(rvest)

# Politely asking for permission to scrape

url <-"https://www.financialexpress.com/about/smart-cities/ "

session <- RCurl::getURL(url, 
                        useragent = str_c(R.version$platform,
                                          R.version$version.string,
                                          sep = ", "),
                        httpheader = c(From = "barosa.carola@gmail.com"))
 
```

Let's then download the titles of the articles:

```{r message=FALSE, warning=FALSE}
# Scraping the titles 

page <- read_html(x = "https://www.financialexpress.com/about/smart-cities/")
nodes <- html_nodes(x = page, css = "h3 a")
knitr::kable(html_text(x = nodes))

```

Downloading and storing the articles' links (from page 1 to 5) in a newly created directory in my project's folder:

```{r message=FALSE, warning=FALSE}

#downloading and getting the links

link_to_pages <- str_c("https://www.financialexpress.com/about/smart-cities/page/", 1:5)
dir.create("SmartCityArticles")

for(i in seq_along(link_to_pages)) {
  download.file(url = link_to_pages[i], destfile = here::here("SmartCityArticles", str_c("page", i, ".html")))
  Sys.sleep(1)
}

out <- vector(mode = "list", length = 5)

for(i in seq_along(link_to_pages)) {
  out[[i]]<-read_html(x = here::here("SmartCityArticles", str_c("page", i, ".html")))%>%
    html_nodes(css="h3 a")%>%
    html_attr("href")
}

knitr::kable(out [[1]])

```

Scraping the main text I'll need:

```{r message=FALSE, warning=FALSE}
#scraping the articles previews texts

text<- rep(list(vector(mode="list", length = 27)), 5)

for(z in 1:5){
  text[[z]][1:27] <- read_html(link_to_pages[z]) %>% 
    html_nodes("h4") %>% 
    html_text()
}

knitr::kable(head((text [[1]])))

```

Scraping the date of publication for each article in each page:

```{r message=FALSE, warning=FALSE}

# getting the publication dates 

date <- rep(list(vector(mode="list", length = 27)), 5)

for(t in 1:5){
  date[[t]][1:27] <- read_html(link_to_pages[t]) %>% 
    html_nodes(".minsago") %>% 
    html_text()
}

knitr::kable((date)[[1]])
```

Creating a data frame in which to store the texts and the dates of publication:

```{r}

texts <- unlist(text)
dates <- unlist(date)

articles_df <- data.frame(texts, dates, stringsAsFactors = FALSE)

articles_df


```

Tidying the "*dates*" variable using the package *lubridate:*

```{r}

library(lubridate)

articles_df$dates <- mdy(articles_df$dates)

head(articles_df$dates)


```

### Step 3: Saving the texts in a .csv file

Saving the data frame in a .csv file called *articles_docs.csv.*

```{r message=FALSE, warning=FALSE}
articles_docs <- write.csv(articles_df,"articles_docs.csv", row.names = TRUE)

```

## Creating and analyzing the corpus 

### Corpus and the Document-feature matrix

Loading the required packages:

```{r message=FALSE, warning=FALSE}

# Loading the required packages 

library(quanteda)
library(quanteda)
library(ggplot2)
library(quanteda.textstats)

```

Reading the *articles_docs.csv* file to create the corpus:

```{r}
x <- read.csv(file = "articles_docs.csv")
```

Creating the corpus:

```{r}
articles_corpus <- corpus(
  x,
  docid_field = "doc_id",
  text_field = "texts",
  meta = list(),
  unique_docnames = TRUE
)

knitr::kable(head(summary(articles_corpus)))
```

Creating the Document-features matrix by removing english stopwords, setting each text in lower case, removing punctuation and numbers.

```{r}
# creating the dfm

myDfm <- dfm(articles_corpus, remove = stopwords("english"), tolower = TRUE, 
             remove_punct = TRUE, remove_numbers=TRUE)

knitr::kable(head(myDfm)[,1:8])

```

### Most occurent words within the corpus

Top 10 words:

```{r}
knitr::kable(topfeatures(myDfm , 10)) 

```

Top features frequency:

```{r}
features_dfm <- textstat_frequency(myDfm, n = 20)

ggplot(features_dfm, aes(x = feature, y = frequency)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

Displaying a wordcloud:

```{r message=FALSE, warning=FALSE}
set.seed(100)
textplot_wordcloud(myDfm , min.count = 6, random.order = FALSE,
                   rot.per = .25, 
                   colors = RColorBrewer::brewer.pal(8,"Dark2"))

```

### Applying the NRC sentiment dictionary to the corpus

These next step will we ought to inspecting which are the

```{r message=FALSE, warning=FALSE, results='hide'}
library(syuzhet)

get_sentiment_dictionary(dictionary = 'nrc', language = "english")
nrc_vector <- get_sentiment(articles_corpus , method="nrc")
head(nrc_vector)
```

Applying the NRC vector to the corpus:

```{r}

nrc_data_PR <- get_nrc_sentiment(articles_corpus, language = "english")

knitr::kable(head(nrc_data_PR))

```

Plotting the sentiment:

```{r}
barplot(
  sort(colSums(prop.table(nrc_data_PR[, 1:10]))),
  horiz = TRUE,
  cex.names = 0.7,
  las = 1,
  main = "Emotions", xlab="Frequency")
```

Let's now display in a wordcloud the most used words for the two extreme sentiment labels I have identified: the ones that express positivity and those that express negativity:

Positive words:

```{r message=FALSE, warning=FALSE}

set.seed(100)
wordcloud(articles_df$text[nrc_data_PR$positive] , random.order = FALSE,
          rot.per = .25,  min.count = 2,
          colors = RColorBrewer::brewer.pal(8,"Dark2"))
```

Negative words:

```{r message=FALSE, warning=FALSE}
set.seed(100)
wordcloud(articles_df$text[nrc_data_PR$negative] , random.order = FALSE,
          rot.per = .25,  min.count = 2,
          colors = RColorBrewer::brewer.pal(8,"Dark2"))

```

No difference is observed between the displayed positive and negative words, even though the corpus was analyzed by applying the NRC dictionary, which is able to recognize difference in sentiment within a selected corpus. This lack could be addressed to two main possible reasons: the first provides an explanation lying in the short dimension of the texts, since they're article previews composed of just few sentences each; it is therefore maybe due to the reduced lexical variety of which the short synopses are made of that the NRC algorithm failed labeling correctly the sentiment across texts . The second, instead, addresses this result to the homogeneity of provenience of the articles in corpus: they're in fact all part of the same side of the debate. This may logically lead also to an homogeneity of lexicon and sentiment, since all the texts were written taking into account the same point of view within the debate. The most frequent words are in fact positive and optimist ones, letting us infer the discourse develops almost entirely around the perception of smart city as positive and optimal solutions for urban planning in Indian cities. Moreover, the articles were, for the vast majority, written in 2016: further evidence is therefore provided in favor of the greater presence of lexicon linked to a highly positive sentiment, since the *Smart Cities Mission* was started between those years and its first steps were followed with great enthusiasm.

# Analyzing citizens' voice: a look into TWITTER

```{r include=FALSE}
## store api keys (these are fake example values; replace with your own keys)
api_key <- "0jiORnKSCseYdFtWxZJmF6gQb"
api_secret_key <- "ErHwCr8t6NWtyoBk7x6p9SiyygWT8gppK7HA14m6bzj0wTQ11M"
access_token <- "1350756992101588992-lJaVtzQWLPDq2ju67W5fYH2Rup5ODw"
access_token_secret <- "4WXg3cv1VnQU7lggcpx65mzhVdNw6INg3kyPlwkc4UILZ"

## authenticate via web browser
token <- create_token(
  app = "carolsnake_dev",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)

get_token()

```

## Running the query via geolocalization 

loading the required packages:

```{r message=FALSE, warning=FALSE}

library(rtweet)
library(readtext)
library(quanteda)
library(ggplot2)
library(ggmap)
library(httpuv)
library(maps)
library(leaflet)


```

Running the query looking for tweets written in India containing the words "smart" and "city" :

```{r eval=FALSE, message=FALSE, warning=FALSE}

# Retriving the tweets via geolocalization interacting with the Google Maps API

rt <- search_tweets( "smart city", n = 1000, lang = "en", include_rts = FALSE, geocode = lookup_coords("india"))

# Adding latitude and longitude 

rtll <- lat_lng(rt)

# Saving the results of the query in a .csv file 

write_as_csv(rtll, "rtll.csv", prepend_ids = TRUE, na = "", 
             fileEncoding = "UTF-8")
```

```{r}

# Reading the file in which data are stored

sctwitter <- read.csv("rtll.csv", fileEncoding = "UTF-8")

head(sctwitter)[,1:4]

```

Displaying the first six tweets' texts:

```{r}
head(sctwitter)[,5]
```

The tweets were written in the temporal span shown below:

```{r}
since <- sctwitter$created_at[100]
latest <- sctwitter$created_at[1]
cat("Twitter data","\n",paste("From:",since),"\n",paste(" To:",latest))

```

Plotting the geocoded tweets using *Leaflet*:

```{r message=FALSE, warning=FALSE}

m2 <- leaflet(rtll)
m2 <- addTiles(m2) 
m2 <- addMarkers(m2, lng=sctwitter$lng, lat=sctwitter$lat, popup=sctwitter$text)
m2


```

Displaying the most frequent hashtags:

```{r message=FALSE, warning=FALSE}

ht <- str_extract_all(sctwitter$text, '#[A-Za-z0-9_]+')
ht <- unlist(ht)
knitr::kable(head(sort(table(ht), decreasing = TRUE)))

```

## Creating and analyzing the corpus

### Corpus and Document-Feature Matrix

```{r}

# Creating the corpus 

Twittercorp <- corpus(sctwitter)

#Creating the Dfm

to_remove <- c("<", ">", "+", "-")

myDfmtwitt <- dfm(Twittercorp, remove = stopwords("english"), remove_punct = TRUE, remove_numbers=TRUE, tolower = TRUE, stem = TRUE, remove_url = TRUE)%>%
  dfm_remove(to_remove)


knitr::kable(head(myDfmtwitt)[,1:8])
```

### Most occurrent words within the corpus 

Top 10 words:

```{r}
knitr::kable(topfeatures(myDfmtwitt , 10)) 

```

Displaying features' frequency in a wordcloud:

```{r message=FALSE, warning=FALSE}

set.seed(100)
textplot_wordcloud(myDfmtwitt , min.count = 6, random.order = FALSE,
                   rot.per = .25, 
                   colors = RColorBrewer::brewer.pal(8,"Dark2"))



```

### Applying the NRC Sentiment dictionary 

# Conclusions

# References
